{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pepsi.evaluation.ipynb\n",
        "# by: Tim Lucas Halt (MatrNr. 6682645)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vorbereitung\n",
        "Zunächst werden die notwendigen Bibliotheken installiert und importiert. Möglicherweise ist ein Neustart des Kernels erforderlich."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItsUcrMYYmoP",
        "outputId": "f35e9346-1b2a-46d9-ad5e-de444dca58f5"
      },
      "outputs": [],
      "source": [
        "%pip install scipy\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install seaborn\n",
        "%pip install tensorflow\n",
        "%pip install sweetviz\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import sweetviz as sw\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Laden des Datensatzes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8vkgQlrHCS8",
        "outputId": "bb0ccb21-1b82-41bf-f325-14a50dbefda0"
      },
      "outputs": [],
      "source": [
        "# load the schiller text\n",
        "path = \"https://raw.githubusercontent.com/schutera/tiny_schiller/main/tiny_schiller.txt\"\n",
        "filepath = tf.keras.utils.get_file(\"schiller.txt\", path)\n",
        "with open(filepath) as f:\n",
        "    schiller_text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_9xc4evoZAes"
      },
      "outputs": [],
      "source": [
        "# generate tokens on charakter level\n",
        "tokenizer=keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(schiller_text)\n",
        "\n",
        "max_id=len(tokenizer.word_index)\n",
        "dataset_size=tokenizer.document_count\n",
        "[encoded]=np.array(tokenizer.texts_to_sequences([schiller_text]))-1\n",
        "\n",
        "train_size=dataset_size*80//100\n",
        "dataset=tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "\n",
        "n_steps=100\n",
        "window_length=n_steps+1\n",
        "dataset=dataset.repeat().window(window_length,shift=1,drop_remainder=True)\n",
        "\n",
        "dataset=dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "batch_size=1024\n",
        "dataset=dataset.shuffle(10000).batch(batch_size)\n",
        "dataset=dataset.map(lambda windows: (windows[:,:-1],windows[:,1:]))\n",
        "dataset=dataset.map(lambda X_batch,Y_batch: (tf.one_hot(X_batch,depth=max_id),Y_batch))\n",
        "dataset=dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Definition und Training des Netzes\n",
        "Um Zusammenhänge zwischen verschiedenen Worten zu bewahren wären LSTM und GRU Layer verwendet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "3lS-BPmqZPS_"
      },
      "outputs": [],
      "source": [
        "model=keras.models.Sequential([\n",
        "    keras.layers.LSTM(256, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.LSTM(256, return_sequences=True),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation='softmax'))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeEMzxhuZTzs",
        "outputId": "00f1a7a3-fc4b-4e5a-90e8-08fc98534f18"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
        "history=model.fit(dataset,\n",
        "                  steps_per_epoch=train_size/batch_size,\n",
        "                  epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Textgenerierung\n",
        "Zuerst werden die Functionen definiert, die die den Text verfollständigen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZV9Yy9VSZgWy"
      },
      "outputs": [],
      "source": [
        "def preprocess(texts):\n",
        "    X=np.array(tokenizer.texts_to_sequences(texts))-1\n",
        "    return tf.one_hot(X,max_id)\n",
        "\n",
        "def next_char(text,temperature=1,model=model):\n",
        "    X_new=preprocess([text])\n",
        "    y_proba=model.predict(X_new)[0,-1:,:]\n",
        "    rescaled_logits=tf.math.log(y_proba)/temperature\n",
        "    char_id=tf.random.categorical(rescaled_logits,num_samples=1)+1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "def complete_text(text,n_chars=50,temperature=0.1):\n",
        "    for _ in range(n_chars):\n",
        "        text+=next_char(text,temperature)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anschließend kann ein Text, ausgehend von einem vordefiniertem Wort/Satz generiert werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def complete_text(text,n_chars=50,temperature=0.01):\n",
        "    for _ in range(n_chars):\n",
        "        text+=next_char(text,temperature,model)\n",
        "    return text\n",
        "\n",
        "print(\"Some predicted texts for word 'shakespeare' are as follows:\\n \")\n",
        "print(complete_text('shakespeare'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scheinbar hat das drunkenSchiller Modell, das in dem Git-Repository gespeichert ist eine Vorliebe für Geschichte..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load a saved drunkenschiller\n",
        "model_name = '../Code/model/drunkenSchiller.h5' \n",
        "model_reloaded = tf.keras.models.load_model(model_name)\n",
        "\n",
        "def complete_text(text,n_chars=50,temperature=0.01):\n",
        "    for _ in range(n_chars):\n",
        "        text+=next_char(text,temperature,model_reloaded)\n",
        "    return text\n",
        "\n",
        "print(\"Some predicted texts for word 'Geschichte' are as follows:\\n \")\n",
        "print(complete_text('Geschichte'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
