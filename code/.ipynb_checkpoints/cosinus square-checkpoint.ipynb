{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import display, clear_output\nimport numpy as np\nimport time\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport pandas as pd\nfrom scipy.spatial import distance\n\nimport tensorflow as tf\nprint('Tensorflow version:', tf.__version__, '\\n')\n\n# Keras is a model-level library, meaning that it is built upon tensorflow (using it as a backend) - allowing for\n# high-level building blocks. Making it even easier to design neural networks.\n# We will access it as tf.keras\n\n# The tf and k abbreviations are best practice (same for numpy np and pandas pd),\n# since you do not want to type T E N S O R F L O W all over your code.\n# They are prevalent all over the industry and academia in a way that you'll risk a fight if you import them differently.\n\n# Loading the MNIST dataset in one line\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\nx_train_normalized = x_train/255\nx_test_normalized = x_test/255\n\n# Berechnung der ähnlichsten Ziffern für jede Klasse von 0 bis 9\nfor digit in range(10):\n    # Filtern der Ziffern nach ihrer Klasse\n    class_images = train_images[train_labels == digit]\n\n    # Berechnung der durchschnittlichen Cosinus-Ähnlichkeit für jede Ziffer zu anderen Ziffern derselben Klasse\n    similarities = []\n    for i, image in enumerate(class_images):\n        avg_similarity = 0\n        for other_image in class_images:\n            if not np.array_equal(image, other_image):\n                # Umwandlung von 28x28 Bildern in Vektoren für Cosinus-Ähnlichkeit\n                image_vector = image.flatten()\n                other_image_vector = other_image.flatten()\n                # Berechnung der Cosinus-Ähnlichkeit\n                cosine_similarity = 1 - distance.cosine(image_vector, other_image_vector)\n                avg_similarity += cosine_similarity\n        avg_similarity /= len(class_images) - 1  # Durchschnittliche Ähnlichkeit zu allen anderen Ziffern der Klasse außer sich selbst\n        similarities.append((i, avg_similarity))\n\n    # Sortieren nach der durchschnittlichen Ähnlichkeit und Auswahl der ähnlichsten Ziffer\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    most_similar_index = similarities[0][0]\n\n    most_similar_digit = class_images[most_similar_index]\n\n    # Anzeige der ähnlichsten Ziffer für jede Klasse\n    plt.subplot(2, 5, digit + 1)\n    plt.imshow(most_similar_digit, cmap='gray')\n    plt.title(f'Most similar {digit}')\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# in the next step, we also need to reshape our input to fit our input layer later on.\n# This is due to keras expecting a definition for how many channels your input sample has, as we\n# deal with gray scale this is 1.\nx_train= x_train_normalized.reshape(-1, 28, 28, 1)\nx_test = x_test_normalized.reshape(-1, 28, 28, 1)\n\n# Printing the shape\nprint('x_train:', x_train.shape)\nprint('y_train:', y_train.shape)\nprint('x_test:', x_test.shape)\nprint('y_test:', y_test.shape)","metadata":{"id":"_0eMI4k9bCZF","outputId":"dffad165-ad63-48e0-b822-029da63aaf40","execution":{"iopub.status.busy":"2023-11-30T06:45:40.378333Z","iopub.execute_input":"2023-11-30T06:45:40.379216Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Tensorflow version: 2.13.0 \n\n","output_type":"stream"}]},{"cell_type":"code","source":"# This is the moment where you define your model's architecture\n\npepsi = tf.keras.models.Sequential([\n    tf.keras.layers.InputLayer(input_shape=(28,28,1)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.GaussianNoise(0.75),\n    tf.keras.layers.Dense(10, activation='softmax', use_bias=False)\n    ])\n\npepsi.summary()\n\n# Define your loss\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n# print(-tf.math.log(1/10))\n\nsampleID = 100\nloss_fn(y_train[:1], pepsi(x_train[sampleID-1:sampleID]).numpy()).numpy()\n\n# Compiling basically means to prepare the training routine for your model which consists of the optimizer,\n# the loss, and the metrics which are to be reported during training\n\npepsi.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),\n              loss= loss_fn,\n              metrics=['accuracy'])","metadata":{"id":"oUziIXh2bCZK","outputId":"bd3fa80b-f167-4041-ca82-bd843d012576","execution":{"iopub.status.busy":"2023-11-30T06:39:21.902818Z","iopub.execute_input":"2023-11-30T06:39:21.903214Z","iopub.status.idle":"2023-11-30T06:39:22.056015Z","shell.execute_reply.started":"2023-11-30T06:39:21.903183Z","shell.execute_reply":"2023-11-30T06:39:22.054702Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Model: \"sequential_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_6 (Flatten)         (None, 4)                 0         \n                                                                 \n dropout_13 (Dropout)        (None, 4)                 0         \n                                                                 \n gaussian_noise_6 (Gaussian  (None, 4)                 0         \n Noise)                                                          \n                                                                 \n dense_10 (Dense)            (None, 10)                40        \n                                                                 \n=================================================================\nTotal params: 40 (160.00 Byte)\nTrainable params: 40 (160.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(-tf.math.log(1/10))\u001b[39;00m\n\u001b[1;32m     17\u001b[0m sampleID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 18\u001b[0m loss_fn(y_train[:\u001b[38;5;241m1\u001b[39m], \u001b[43mpepsi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampleID\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43msampleID\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compiling basically means to prepare the training routine for your model which consists of the optimizer,\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# the loss, and the metrics which are to be reported during training\u001b[39;00m\n\u001b[1;32m     23\u001b[0m pepsi\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mlegacy\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[1;32m     24\u001b[0m               loss\u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[1;32m     25\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/input_spec.py:298\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"sequential_10\" is incompatible with the layer: expected shape=(None, 2, 2, 1), found shape=(1, 28, 28, 1)"],"ename":"ValueError","evalue":"Input 0 of layer \"sequential_10\" is incompatible with the layer: expected shape=(None, 2, 2, 1), found shape=(1, 28, 28, 1)","output_type":"error"}]},{"cell_type":"code","source":"vlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = 0.8, patience =2, min_lr=0.00001)\nvlr2 = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n\ner = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_accuracy\",\n        patience=1,\n        restore_best_weights=True\n)\ncheckpoint_filepath = 'tmp/model.{val_accuracy:.4f}.h5'\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    save_freq= 'epoch'\n)\n\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            rotation_range=15,  \n            zoom_range = 0.15,\n            shear_range=0.05,\n            width_shift_range=0.1, \n            height_shift_range=0.1,\n            rescale=0,\n            fill_mode = 'nearest',\n            horizontal_flip=False,\n            vertical_flip=False)\ndatagen.fit(x_train)\n\nhistory = pepsi.fit(\n    datagen.flow(x_train, y_train, batch_size = 32),\n   # x_train,\n    #y_train,\n   # batchsize = 256,\n    epochs=200,\n    validation_data=(x_test, y_test),\n    callbacks=[model_checkpoint]#, vlr]\n)\n(test_loss, test_acc) = pepsi.evaluate(x_test, y_test)\nprint(\"Loss: \", test_loss)\nprint(\"Accuracy: \", test_acc)\n\nmodel_name = 'tmpt/model.{test_acc:.4f}.h5'\npepsi.save(model_name, save_format='h5')\n\nplt.figure(figsize=(13, 5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Test'])\nplt.grid()\nplt.show()","metadata":{"id":"X8d40pU-bCZL","outputId":"f164028f-9ff1-4b5a-c748-e2750b5cba3e","execution":{"iopub.status.busy":"2023-11-30T06:38:37.219587Z","iopub.execute_input":"2023-11-30T06:38:37.220469Z","iopub.status.idle":"2023-11-30T06:38:37.607536Z","shell.execute_reply.started":"2023-11-30T06:38:37.220432Z","shell.execute_reply":"2023-11-30T06:38:37.606290Z"},"trusted":true},"execution_count":34,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 31\u001b[0m\n\u001b[1;32m     18\u001b[0m datagen \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mImageDataGenerator(\n\u001b[1;32m     19\u001b[0m             rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,  \n\u001b[1;32m     20\u001b[0m             zoom_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.15\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m             horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m             vertical_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m datagen\u001b[38;5;241m.\u001b[39mfit(x_train)\n\u001b[1;32m     30\u001b[0m history \u001b[38;5;241m=\u001b[39m pepsi\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     32\u001b[0m    \u001b[38;5;66;03m# x_train,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#y_train,\u001b[39;00m\n\u001b[1;32m     34\u001b[0m    \u001b[38;5;66;03m# batchsize = 256,\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     36\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test),\n\u001b[1;32m     37\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[model_checkpoint]\u001b[38;5;66;03m#, vlr]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m (test_loss, test_acc) \u001b[38;5;241m=\u001b[39m pepsi\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, test_loss)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/preprocessing/image.py:1545\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[0;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow\u001b[39m(\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1488\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1499\u001b[0m ):\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes data & label arrays, generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \n\u001b[1;32m   1544\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNumpyArrayIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_class_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_class_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/preprocessing/image.py:707\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[1;32m    704\u001b[0m     x_misc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y):\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x` (images tensor) and `y` (labels) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, y.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape, np\u001b[38;5;241m.\u001b[39masarray(y)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    712\u001b[0m     )\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sample_weight):\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x` (images tensor) and `sample_weight` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, sample_weight.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape, np\u001b[38;5;241m.\u001b[39masarray(sample_weight)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    719\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: `x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (11760000, 2, 2, 1), y.shape = (60000,)"],"ename":"ValueError","evalue":"`x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (11760000, 2, 2, 1), y.shape = (60000,)","output_type":"error"}]},{"cell_type":"code","source":"# This line would start up tensorboard for you\n%tensorboard --logdir logs --host localhost","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:55:45.923470Z","iopub.status.idle":"2023-11-07T13:55:45.923825Z","shell.execute_reply":"2023-11-07T13:55:45.923659Z","shell.execute_reply.started":"2023-11-07T13:55:45.923643Z"},"id":"_IuffAeTbCZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# after the training finishes, we will also save Marvin in Keras style (HDF5), so we do not have to\n# train him again\n# every time we start our computer. Obviously, by changing the model_name, you can also save different\n# configurations of Marvin. The name has to be a string, like this: 'name.h5'\nmodel_name = 'pepsi_V03e_99.6'\npepsi.save(model_name, save_format='h5')\n\n# It is best practice to indicate what configuration changes you did within the name, so you know\n# which model you need to load already from its name\n# Let's say instead of a learning rate of 0.001 you used 0.1, your naming could then look like:\n# 'marvin_lr01.h5'\n\nprint('Success! You saved Marvin as: ', model_name)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:55:45.924892Z","iopub.status.idle":"2023-11-07T13:55:45.925202Z","shell.execute_reply":"2023-11-07T13:55:45.925061Z","shell.execute_reply.started":"2023-11-07T13:55:45.925047Z"},"id":"8xB2FVb5bCZM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot for the intuitive approach\n\nnumbers_to_display = 196\nnum_cells = math.ceil(math.sqrt(numbers_to_display))\nplt.figure(figsize=(15, 15))\n\nfor plot_index in range(numbers_to_display):\n    predicted_label = predictions[plot_index]\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    color_map = 'Greens' if predicted_label == y_test[plot_index] else 'Reds'\n    plt.subplot(num_cells, num_cells, plot_index + 1)\n    plt.imshow(x_test_normalized[plot_index].reshape((28, 28)), cmap=color_map)\n    plt.xlabel(predicted_label)\n\nplt.subplots_adjust(hspace=1, wspace=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:55:45.926253Z","iopub.status.idle":"2023-11-07T13:55:45.926586Z","shell.execute_reply":"2023-11-07T13:55:45.926431Z","shell.execute_reply.started":"2023-11-07T13:55:45.926416Z"},"id":"lcYtZp34bCZM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load a saved marvin configuration you want to evaluate\nmodel_name = 'model.77-0.9975.h5'\npepsi_reloaded = tf.keras.models.load_model(model_name)\n\n# Let Marvin predict on the test set, so we have some data to evaluate his performance.\npredictions = pepsi_reloaded.predict([x_test])\n# predictions = pepsi.predict([x_test])\n\n# Remember that the prediction of Marvin is a probability distribution over all ten-digit classes\n# We want him to assign the digit class with the highest probability to the sample.\npredictions = np.argmax(predictions, axis=1)\n#pd.DataFrame(predictions)","metadata":{"id":"UCKhienmbCZM","outputId":"ab1c8851-b794-4f29-e1cc-d645120419b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n(test_loss, test_acc) = pepsi_reloaded.evaluate(x_test, y_test)\nprint(\"Loss: \", test_loss)\nprint(\"Accuracy: \", test_acc)\n\nconfusion_matrix = tf.math.confusion_matrix(y_test, predictions)\n\nf, ax = plt.subplots(figsize=(9, 7))\nsn.heatmap(\n    confusion_matrix,\n    annot=True,\n    linewidths=.7,\n    fmt=\"d\",\n    square=True,\n    ax=ax,\n    cmap=\"viridis\",\n)\nplt.show()","metadata":{"id":"NQPYK-B0bCZM","outputId":"657ce5a1-82e4-4a12-83d2-bae2716ac39c"},"execution_count":null,"outputs":[]}]}